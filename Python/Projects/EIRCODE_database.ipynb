{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2187ace3-b6f9-43b9-9705-c13b8f92fa56",
   "metadata": {
    "tags": []
   },
   "source": [
    "## EIRCODE database creation\n",
    "\n",
    "There are a number of options available to review. The long term aim is to develop an EIRCODE database that includes sufficient detail to map the location details into a map.\n",
    "\n",
    "Data source:\n",
    "- Property price register [website](https://www.propertypriceregister.ie/) provides an option to download all data from database into zip file\n",
    "- EIRCODE API [website](https://services.vision-net.ie/eircode.jsp). Includes a JSON format output. An Póst Geo ID also included.\n",
    "- GeoCoding APIs\n",
    "> - Google API [website](https://developers.google.com/maps/documentation/geocoding/overview)\n",
    "> - MapQuest [website](https://www.mapquest.com/) with a developer API option for geocoding [website](https://developer.mapquest.com/documentation/geocoding-api). NOTE that a pricing plan is in place per monthly transactions. Requires personal API key to request details. \n",
    "\n",
    "Objectives:\n",
    "- Review the PPR database to understand volume of properties with non-null EIRCODE\n",
    "- Review recent sample of PPR properties with property websites to see if data for EIRCODE is still available\n",
    ">- Develop an automated search function\n",
    "- Location details [LAT, LNG]\n",
    ">- Take sample of properties and extract LAT,LNG from range of sources e.g., Google MAP API, to understand what can be extracted\n",
    "\n",
    "Backlog:\n",
    "- Refactor to use modins API, allow for parallel computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e2535-d9de-48cc-8eb6-9336530d3a48",
   "metadata": {},
   "source": [
    "### Additional NLP processing\n",
    "\n",
    "Aim was to review a number of algorithms that could help benchmark the matching algorithms taking place. Using the EIRCODE dataset that has been formatted to understand what is possible. \n",
    "\n",
    "Next steps:\n",
    "- Create a sample dataset with fake data entry issues e.g., misspelling, missing information, different format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6d46e-6563-4894-bb47-179099e132d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required modules\n",
    "from zipfile import ZipFile\n",
    "  \n",
    "# specifying the zip file name - maintain file format that is downloaded from the PPR website. Algorithm below will unzip to working directory\n",
    "file_name = \"PPR-ALL.zip\"\n",
    "  \n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(file_name, 'r') as zip_file:\n",
    "    # printing all the contents of the zip file\n",
    "    zip_file.printdir()\n",
    "  \n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    zip_file.extractall()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf8583-4158-4a1a-bf9d-d7f633cfb9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import sys\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae732da-3a63-470d-82c1-6a6fdfdd788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review number of cpus available - this is where the introduction of modin could aid with parallel processing\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ac498-61e3-4920-b01d-b74a2abe9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook setting updates\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Adjust options for displaying the float columns\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# Warning settings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba89ad-bc37-47a6-98a9-82d30b7567ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ppr data\n",
    "df = pd.read_csv('PPR-ALL.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1c623-0c1a-4b3e-a7c4-e6d6428b867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be043d3-3cd6-4bb2-bc0c-35fb4ee55469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to clean imported data to allow for exploration. Code is taken from the analysis completed in section 1 of this notebook. For interactive insight on steps user can re-run outputs.\n",
    "# .assign : steps used to update variables\n",
    "def tweak_jb(df):\n",
    "    return (\n",
    "        df\n",
    "        .rename(columns=lambda c:c.replace(' ','_'))\n",
    "        .rename(columns={'Date_of_Sale_(dd/mm/yyyy)':'Date_of_Sale',\n",
    "                        'Price_()':'Price'\n",
    "                       })\n",
    "        .assign(Price=lambda df_:df_.Price.str[1:].str.replace(',','').astype(float),\n",
    "                Date_of_Sale=lambda df_:pd.to_datetime(df_.Date_of_Sale),\n",
    "                Not_Full_Market_Price=lambda df_:df_.Not_Full_Market_Price.astype('category'),\n",
    "                VAT_Exclusive=lambda df_:df_.VAT_Exclusive.astype('category'),\n",
    "                Description_of_Property=lambda df_:df_.Description_of_Property.astype('category'),\n",
    "                Property_Size_Description=lambda df_:df_.Property_Size_Description.astype('category'),\n",
    "               )\n",
    "        .drop(columns=['Property_Size_Description'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e76e14-b109-4a31-a2cc-d6601c1092ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function to create the updated DataFrame for analysis\n",
    "df1 = tweak_jb(df)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1a0f3-7ccd-47e0-84bd-0936cff5d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing frequency for each feature\n",
    "(\n",
    "    df1\n",
    "    # .Property_Size_Description # remove not enough info\n",
    "    .Description_of_Property\n",
    "    # .VAT_Exclusive\n",
    "    # .Not_Full_Market_Price\n",
    "    .value_counts(dropna=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af22df-b38e-4422-815f-db41e0efb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review high level details from DataFrame\n",
    "df1.describe(include='all').T # check for cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39119c85-991a-462c-acea-f32e04faa2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the columns can be converted to categories. If there is a low cardinality (proportion of unique values) then it \n",
    "# makes sense to convert the column data type\n",
    "cardinality = df1.apply(pd.Series.nunique) # Display the cardinality for each column\n",
    "cardinality\n",
    "\n",
    "# Extract the column name which matches the column index value being reviewed\n",
    "N = 6\n",
    "cat_val = [i for i in (df1.apply(pd.Series.nunique)) if i <= N]\n",
    "cat_cols = [df1.columns[i] for i, n in enumerate(df1.apply(pd.Series.nunique)) if n <= N] # adding the enumerate method provides an index value\n",
    "cat_val\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8060394-9bab-4240-be1f-070a5e3bed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a7399-7c04-4dc3-aced-2028c70f79a2",
   "metadata": {},
   "source": [
    "### 2 Missing value review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5dd12-916b-41d3-96b0-015a4c170646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the missing values by column\n",
    "df1.isnull().sum()\n",
    "\n",
    "# Create method to review the proportion of missing values by each column\n",
    "def missing_columns(df):\n",
    "    for col in df.columns:\n",
    "        miss = df.isnull().sum()\n",
    "        miss_per = miss / len(df)\n",
    "    return miss_per\n",
    "\n",
    "missing_columns(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612e999-9902-42c8-ad24-9a2d2df07b82",
   "metadata": {},
   "source": [
    "### Review EIRCODEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd3c3d-9223-4e52-a4ee-dd2ccea99bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review number of non-null EIRCODEs within DataFrame\n",
    "df_e = df1.loc[df1.Eircode.notna()]\n",
    "df_e.shape\n",
    "df_e.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7d871-7c3b-4842-b1be-86b31db7dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition of KW parameter provides output for non-numeric features\n",
    "df_e.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a81b9-024d-4305-8a5b-b46dc606106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review price variable\n",
    "df_e['Price'].describe(percentiles=[.1, .2, .3, .4, .5, .6 , .7, .8, .9, .95, .99, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a73e9-33ab-422b-ab41-ccc0a14ea4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.hist(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e69b3-330d-44f6-8a04-005dea43c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly review provides interactive options\n",
    "fig = px.histogram(df_e, x=\"Date_of_Sale\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c22295-0087-44c1-b076-70da796e12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude larger outliers\n",
    "fig = px.histogram(df_e.loc[(df_e.Price < 800_000)], x=\"Price\", nbins=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4d5e7-e0be-442a-8737-450a873d152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "dups_check = df_e.Eircode.is_unique\n",
    "dups_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b021e-74ae-4228-83a4-9fdfa5767cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review duplicates by EIRCODE feature\n",
    "df_e['duplicate'] = df_e.duplicated(keep=False, subset=['Eircode'])\n",
    "df_e.head()\n",
    "dup_count = df_e['duplicate'].value_counts().to_string()\n",
    "dup_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810b84c-d65c-4f2a-83ee-1cfa01e978cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand volume of duplicates \n",
    "duplicates = (\n",
    "    df_e\n",
    "    .loc[df_e.duplicate == True]\n",
    "    # .head()\n",
    "    .sort_values(['Eircode'])\n",
    "    # .head(20)\n",
    "    .groupby(['Eircode'])['Eircode']\n",
    "    .count()\n",
    "    .value_counts() # include to check numbers by duplicate category\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "duplicates\n",
    "# check for highest number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9f543-f597-45e7-8fe7-ec630c593a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single sale date, multiple entries\n",
    "df_e_check = df_e.loc[df_e.Eircode == 'D07F6K5']\n",
    "df_e_check\n",
    "df_e_check.groupby('Eircode').agg({'Price': sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b20f48-f512-4491-94fe-673f68b83cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate sale dates\n",
    "df_e_check1 = df_e.loc[df_e.Eircode == 'N39TP27']\n",
    "df_e_check1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba513c-acfc-49e4-a42e-f50d9d0b81d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (\n",
    "    df_e\n",
    "    .loc[df_e.Eircode == 'D11XE43']\n",
    ")\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5924350d-010e-4cc3-a400-cd8e002eee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = (\n",
    "    df1\n",
    "    .loc[df1.Address.str.contains('BELCLARE PARK', case=False)]\n",
    ")\n",
    "sample1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0c2d8-f031-431d-bd58-7b156485458f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Next steps\n",
    "\n",
    "- Try GeoCoding APIs to understand how additional EIRCODEs could be retreived\n",
    "- Use analysis to review recent property sales i.e., most recent 3 months, could have EIRCODEs missing and how to start providing details. Having an understanding of details that could be seen within the property websites e.g., daft, estate agents, could help with this review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba346058-efd4-40be-aa34-f722c12e9529",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46d530-9b86-4b97-ac81-13c0c703dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e520b0-ef5c-46e2-a7d0-ebac909f7502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23117f9b-adf8-41c4-aa2d-549cc250132b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "# Example sentence\n",
    "address = \"UNIT 1 69 CABRA RD, PHIBSBOROUGH, DUBLIN 7\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = address.split()\n",
    "\n",
    "# Create bigrams from the list of words\n",
    "bigrams = ngrams(words, 2)\n",
    "\n",
    "# Print the bigrams\n",
    "for bigram in bigrams:\n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0206e-2366-4094-8ecd-a3b2be52395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = ngram.NGram(['joe','joseph','jon','john','sally'])\n",
    "G.search('jon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b361f8-6e99-43c2-a041-2a05431d44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_str = 'dublin'\n",
    "\n",
    "for bigram in bigrams:\n",
    "    print(ngram.NGram.compare(bigram, check_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c87462-f371-4aba-8a6b-2d82dcc25593",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Search Engine review\n",
    "\n",
    "Working with BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaedf67-359a-4f63-aa1e-e2e1494c4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# import installPack\n",
    "\n",
    "# Function to review and install package if missing\n",
    "def installPackage(package):\n",
    "    p = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", package], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    print(p.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fca745-7049-4e4a-ba2b-a79a08eade29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of libraries to import\n",
    "requirements = [\"spacy\", \"rank_bm25\"]\n",
    "for requirement in requirements:\n",
    "    # installPack.installPackage(requirement)\n",
    "    installPackage(requirement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128b8d1-fcbd-4c19-93c6-6f24a0d39d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65595701-08ea-4d23-b3b1-32e97f15d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise SpaCy - getting errors so can't compute these steps\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9d839-c0fe-427f-8f61-ec3cd13b3d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address will be the string\n",
    "df_e.Address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2c0d1-71fa-4ea7-9173-3c97dbe83399",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = df_e.Address.str.lower().values\n",
    "tok_text = [] # tokenised corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee26dab-6026-4426-b84d-61b6ee9e0af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b3c9d-3c54-4f38-8f85-94460b7004c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenising using SpaCy\n",
    "# for doc in tqdm(nlp.pipe(text_list, disable=[])):\n",
    "#     tok = [t.text for t in doc if t.is_alpha]\n",
    "#     tok_text.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed441bc-8824-4922-9cd6-f457254fd40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0b0db-9be4-44e4-9fed-0a8706ec65f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy Matching\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "similarity = fuzz.ratio(\"UNIT 6 69 CABRA RD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\", \"UNIT 6 69 CABRA ROAD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\")\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea5d44-2f3b-4294-9639-78a1ce470dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class statement to work with on sample addresses\n",
    "class FuzzyStringMatcher:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def calculate_similarity_score(self, source_column, target_column, new_column_name):\n",
    "        self.df[new_column_name] = self.df.apply(lambda row: fuzz.ratio(row[source_column], row[target_column]), axis=1)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Sample DataFrame\n",
    "    data = {'Address1': [\"UNIT 6 69 CABRA RD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\", \"UNIT 1 69 CABRA RD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\", \"UNIT 5 69 CABRA RD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\"],\n",
    "            'Address2': [\"UNIT 4 69 CABRA RoaD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\", \"UNIT 3 69 CABRA RD, PHIBSBORO, DUBLIN 7, D07F6K5\", \"UNIT 7 69 CABRA RD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\"]}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create an instance of the FuzzyStringMatcher class\n",
    "    fuzzy_matcher = FuzzyStringMatcher(df)\n",
    "    \n",
    "    # Calculate similarity score\n",
    "    fuzzy_matcher.calculate_similarity_score(source_column='Address1', target_column='Address2', new_column_name='SimilarityScore')\n",
    "    \n",
    "    # Display DF\n",
    "    fuzzy_matcher.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbb2c5-9d27-4611-9471-72bb7309a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class that works with larger dataframes\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "class FuzzyStringMatcherBest:\n",
    "    def __init__(self, review_df, target_df, review_column, target_column):\n",
    "        self.review_df = review_df\n",
    "        self.target_df = target_df\n",
    "        self.review_column = review_column\n",
    "        self.target_column = target_column\n",
    "        self.best_matches = {}\n",
    "    \n",
    "    def calculate_best_match(self):\n",
    "        for count, review_item in tqdm(enumerate(self.review_df[self.review_column])):\n",
    "            # Find the best match for the review item in the target DataFrame\n",
    "            best_match, score, _ = process.extractOne(review_item, self.target_df[self.target_column])\n",
    "            self.best_matches[count] = {'Review_item': review_item, 'Best_Match': best_match, 'Score': score}\n",
    "        \n",
    "        # Create a DataFrame to store the results\n",
    "        result_df = pd.DataFrame.from_dict(self.best_matches, orient='index')\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Sample DataFrame\n",
    "    review_data = {'Address1': [\"UNIT 6 69 CABRA RD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\", \"UNIT 1 69 CABRA RD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\", \"UNIT 5 69 CABRA RD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\"]}\n",
    "    target_data = {'Address2': [\"UNIT 1 69 CABRA RoaD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\", \"UNIT 3 69 CABRA RD, PHIBSBORO, DUBLIN 7, D07F6K5\", \"UNIT 7 69 CABRA RD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\"]}\n",
    "    review_df = pd.DataFrame(review_data)\n",
    "    target_df = pd.DataFrame(target_data)\n",
    "\n",
    "    # Create an instance of the FuzzyStringMatcher class\n",
    "    fuzzy_matcher = FuzzyStringMatcherBest(review_df, target_df, review_column='Address1', target_column='Address2')\n",
    "    \n",
    "    # Calculate best score\n",
    "    best_matches_df = fuzzy_matcher.calculate_best_match()\n",
    "    \n",
    "    # Display DF\n",
    "    best_matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd4f40-bbb8-4b70-bc99-1758ef6a95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e1 = (\n",
    "    df_e\n",
    "    .assign(address_eir = df_e.Address + ', ' + df_e.Eircode)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55411417-c1d1-4a22-aa62-54ae65fc8364",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Sample DataFrame\n",
    "    review_data = df_e1.sample(n=100, replace=True, random_state=1)\n",
    "    target_data = df_e1.sample(n=500, replace=True, random_state=2)\n",
    "    review_df = pd.DataFrame(review_data)\n",
    "    target_df = pd.DataFrame(target_data)\n",
    "\n",
    "    # Create an instance of the FuzzyStringMatcher class\n",
    "    fuzzy_matcher = FuzzyStringMatcherBest(review_df, target_df, review_column='address_eir', target_column='address_eir')\n",
    "    \n",
    "    # Calculate best score\n",
    "    best_matches_df = fuzzy_matcher.calculate_best_match()\n",
    "    \n",
    "    # Display DF\n",
    "    best_matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a893a14-b305-42c7-b83c-f5ed1fc84275",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_matches_df.Score.value_counts(ascending=True)\n",
    "best_matches_df.loc[(best_matches_df.Score >= 80),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e972209-56d5-47f0-bdd6-9c2bf3297d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e466dba-55e5-4a51-a2c8-ff5f13561438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Text Similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.metrics import jaccard_distance\n",
    "import nltk\n",
    "\n",
    "# had to download punctuation package to allow work_tokenize to work\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01132e-6c90-4f87-b238-9cb1cc63d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform review\n",
    "address1_tokens = set(word_tokenize(\"UNIT 6 69 CABRA RD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\"))\n",
    "address2_tokens = set(word_tokenize(\"UNIT 6 69 CABRA ROAD, PHIBSBOROUGH, DUBLIN 7, D07F6K5\"))\n",
    "\n",
    "similarity = 1 - jaccard_distance(address1_tokens, address2_tokens)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30532368-6fdc-4a6e-aac5-c5ee670448ba",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Analysis will review how effectively TF-IDF can help to optimise searching through a list of addresses to perform matching.\n",
    "\n",
    "Code was originally used by Tim Black to match IMDB movie titles with a MovieLens dataset [article](https://medium.com/tim-black/fuzzy-string-matching-at-scale-41ae6ac452c2). Tim references work completed by Chris van den Berg on TF-IDF approach.\n",
    "\n",
    "We are going to take the code and apply it to the EIRCODE dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eda7e3-50d4-4a89-ac3f-2437d306744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing errors so can't get it working correctly\n",
    "# spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38be6bc-4f48-48aa-9455-f4c1a5549edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sparse_dot_topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbc91a-b9b6-45d1-8ecd-6150bcd67264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import re\n",
    "import time\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "\n",
    "import sparse_dot_topn.sparse_dot_topn as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f97e24e-274b-49ad-8ad1-ef2807e08a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringMatch():\n",
    "    \n",
    "    def __init__(self, source_names, target_names):\n",
    "        self.source_names = source_names\n",
    "        self.target_names = target_names\n",
    "        self.ct_vect      = None\n",
    "        self.tfidf_vect   = None\n",
    "        self.vocab        = None\n",
    "        self.sprse_mtx    = None\n",
    "        \n",
    "        \n",
    "    def tokenize(self, analyzer='char_wb', n=3):\n",
    "        '''\n",
    "        Tokenizes the list of strings, based on the selected analyzer\n",
    "\n",
    "        :param str analyzer: Type of analyzer ('char_wb', 'word'). Default is trigram\n",
    "        :param str n: If using n-gram analyzer, the gram length\n",
    "        '''\n",
    "        # Create initial count vectorizer & fit it on both lists to get vocab\n",
    "        self.ct_vect = CountVectorizer(analyzer=analyzer, ngram_range=(n, n))\n",
    "        self.vocab   = self.ct_vect.fit(self.source_names + self.target_names).vocabulary_\n",
    "        \n",
    "        # Create tf-idf vectorizer\n",
    "        self.tfidf_vect  = TfidfVectorizer(vocabulary=self.vocab, analyzer=analyzer, ngram_range=(n, n))\n",
    "        \n",
    "        \n",
    "    def match(self, ntop=1, lower_bound=0, output_fmt='df'):\n",
    "        '''\n",
    "        Main match function. Default settings return only the top candidate for every source string.\n",
    "        \n",
    "        :param int ntop: The number of top-n candidates that should be returned\n",
    "        :param float lower_bound: The lower-bound threshold for keeping a candidate, between 0-1.\n",
    "                                   Default set to 0, so consider all canidates\n",
    "        :param str output_fmt: The output format. Either dataframe ('df') or dict ('dict')\n",
    "        '''\n",
    "        self._awesome_cossim_top(ntop, lower_bound)\n",
    "        \n",
    "        if output_fmt == 'df':\n",
    "            match_output = self._make_matchdf()\n",
    "        elif output_fmt == 'dict':\n",
    "            match_output = self._make_matchdict()\n",
    "            \n",
    "        return match_output\n",
    "        \n",
    "        \n",
    "    def _awesome_cossim_top(self, ntop, lower_bound):\n",
    "        ''' https://gist.github.com/ymwdalex/5c363ddc1af447a9ff0b58ba14828fd6#file-awesome_sparse_dot_top-py '''\n",
    "        # To CSR Matrix, if needed\n",
    "        A = self.tfidf_vect.fit_transform(self.source_names).tocsr()\n",
    "        B = self.tfidf_vect.fit_transform(self.target_names).transpose().tocsr()\n",
    "        M, _ = A.shape\n",
    "        _, N = B.shape\n",
    "\n",
    "        idx_dtype = np.int32\n",
    "\n",
    "        nnz_max = M * ntop\n",
    "\n",
    "        indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "        indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "        data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "        ct.sparse_dot_topn(\n",
    "            M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "            np.asarray(A.indices, dtype=idx_dtype),\n",
    "            A.data,\n",
    "            np.asarray(B.indptr, dtype=idx_dtype),\n",
    "            np.asarray(B.indices, dtype=idx_dtype),\n",
    "            B.data,\n",
    "            ntop,\n",
    "            lower_bound,\n",
    "            indptr, indices, data)\n",
    "\n",
    "        self.sprse_mtx = csr_matrix((data,indices,indptr), shape=(M,N))\n",
    "    \n",
    "    \n",
    "    def _make_matchdf(self):\n",
    "        ''' Build dataframe for result return '''\n",
    "        # CSR matrix -> COO matrix\n",
    "        cx = self.sprse_mtx.tocoo()\n",
    "\n",
    "        # COO matrix to list of tuples\n",
    "        match_list = []\n",
    "        for row, col, val in zip(cx.row, cx.col, cx.data):\n",
    "            match_list.append((row, self.source_names[row], col, self.target_names[col], val))\n",
    "\n",
    "        # List of tuples to dataframe\n",
    "        colnames = ['Row_Idx', 'Sample_Address', 'Source_Idx', 'Source_Address', 'Score']\n",
    "        match_df = pd.DataFrame(match_list, columns=colnames)\n",
    "\n",
    "        return match_df\n",
    "\n",
    "    \n",
    "    def _make_matchdict(self):\n",
    "        ''' Build dictionary for result return '''\n",
    "        # CSR matrix -> COO matrix\n",
    "        cx = self.sprse_mtx.tocoo()\n",
    "\n",
    "        # dict value should be tuple of values\n",
    "        match_dict = {}\n",
    "        for row, col, val in zip(cx.row, cx.col, cx.data):\n",
    "            if match_dict.get(row):\n",
    "                match_dict[row].append((col,val))\n",
    "            else:\n",
    "                match_dict[row] = [(col, val)]\n",
    "\n",
    "        return match_dict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0352b-1682-4269-a965-f452301ada8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sample list for review\n",
    "def sample_test_df(n=1_000):\n",
    "    return (\n",
    "    df_e\n",
    "    .sample(n=n, random_state=1)\n",
    ")\n",
    "# df_e1 = sample_test_df(10_000)\n",
    "df_e1 = sample_test_df(25_000)\n",
    "df_e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bef99f8-12e3-4042-8b96-793f7aae57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Match the sample address to EIRCODE addresses (and time it)\n",
    "t0 = datetime.now()\n",
    "titlematch = StringMatch(df_e1.Address.tolist(), df_e.Address.tolist()) # first param: sample list, second param: target list\n",
    "titlematch.tokenize()\n",
    "match_df = titlematch.match()\n",
    "t1 = datetime.now()\n",
    "full_time_tfidf = (t1-t0).total_seconds()\n",
    "full_time_tfidf\n",
    "\n",
    "# Performance:\n",
    "# n = 1_000; time = 4.5s\n",
    "# n = 10_000; time = 13s\n",
    "# n = 25_000; time = 28s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc48de-78f5-4498-b9ce-696cbc74dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab9e9a-f0e3-45dd-b4e3-2cb1f514e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should result in perfect match as the same address is being used for both fields (Sample & Source)\n",
    "match_df.groupby(round(match_df.Score,2)).agg({'Sample_Address':['count']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986250f-b14e-4489-bd33-9c75c7013510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the sample input list increased in size the score moved towards 1. Therefore query results only work for sample size (n=1_000)\n",
    "# review = (\n",
    "#     match_df.loc[(round(match_df.Score,2) == 0.98),:]\n",
    "# )\n",
    "# review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ea5ad-5590-4aa8-ac27-0224798b8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to display the matrix output. As sample size has increased this output has increased in size\n",
    "plt.spy(titlematch.sprse_mtx);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218b503-a72c-471d-ba57-f3444dd3fa44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
