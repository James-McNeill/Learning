{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c70a3d-39d4-4d52-9ede-86cf115dd5a3",
   "metadata": {},
   "source": [
    "## Dataset Transition Review (JUN - SEP Transition)\n",
    "\n",
    "Process\n",
    "- Review each of the monthly snapshots to include additional variables\n",
    "- Combine loan level data to understand transitions\n",
    "- Produce summary analysis metrics\n",
    "- Export to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e11970-a8cd-43ad-8421-98ed8928c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import sys\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006b35f-74d8-467f-9277-8b398935e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook setting updates\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Adjust options for displaying the float columns\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# Set options to display all columns\n",
    "pd.set_option('display.max_columns', None)  # None means no limit\n",
    "\n",
    "# Warning settings\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0862a2-071b-41bc-b703-aee798654e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key variables\n",
    "columns_to_import = ['...',] # select subset of columns\n",
    "file_jun = 'file_jun.csv'\n",
    "file_sep = 'file_sep.csv'\n",
    "file_run = 'review'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11b86f-37e2-4cdb-8a6b-0a5fe35ae700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "# df_j = pd.read_csv(file_jun, encoding = \"ISO-8859-1\",) # review initial file to understand column names\n",
    "df_j = pd.read_csv(file_jun, encoding = \"ISO-8859-1\",usecols=columns_to_import)\n",
    "df_s = pd.read_csv(file_sep, encoding = \"ISO-8859-1\",usecols=columns_to_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353c510-5c2e-4b17-b7a6-b766a6f651b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary detail on the imported datasets\n",
    "df_j.info(memory_usage='deep')\n",
    "df_s.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b39e9e-7d66-47b9-943b-830c3f048ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine arrears status\n",
    "def arrears_status(score):\n",
    "    if score >= 53:\n",
    "        return \"53+\"\n",
    "    elif score >= 40:\n",
    "        return \"40-52\"\n",
    "    elif score >= 27:\n",
    "        return \"27-39\"\n",
    "    elif score >= 19:\n",
    "        return \"19-26\"\n",
    "    elif score >= 10:\n",
    "        return \"10-18\"\n",
    "    elif score >= 1:\n",
    "        return \"1-9\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "\n",
    "def coll_rates(score):\n",
    "    if score >= 26:\n",
    "        return 1  \n",
    "    elif score >= 19:\n",
    "        return 0.000    \n",
    "    elif score >= 10:\n",
    "        return 0.000    \n",
    "    elif score >= 1:\n",
    "        return 0.0000    \n",
    "    else:\n",
    "        return 0.0000     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b474ed-300d-478f-ac88-f76c15fee443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review with relation variable details\n",
    "# Function used to clean imported data to allow for exploration.\n",
    "# .assign : steps used to update variables\n",
    "def tweak_jb(df):\n",
    "    return (\n",
    "        df\n",
    "        .rename(columns=lambda c:c.replace(' ','_'))\n",
    "        .rename(columns={'ï»¿LOAN_NO':'Loan_No',\n",
    "                         'LOAN_PRINC':'Balance',\n",
    "                         ...,\n",
    "                   })\n",
    "        # .loc[df.OPEN_FLAG == 'Y']  # Used to define loan open/closed details at timestamp.\n",
    "        .assign(\n",
    "                # Attached_Shares_check=lambda df_:df_[['Outstanding_Balance', 'TOTAL_SAVINGS', '...',]].min(axis=1).clip(lower=0),\n",
    "                Arrears_Band=lambda df_:df_.WeeksInArrears.apply(arrears_status),#.astype('category'),\n",
    "                Loan_Vol=lambda df_:df_.groupby('Borrower_No')['Borrower_No'].transform('size'),\n",
    "                Borrower_Out_Bal=lambda df_:df_.groupby('Borrower_No')['Balance'].transform('sum'),\n",
    "                Purpose_Count=lambda df_:df_.groupby(['Borrower_No'])['Purpose'].transform('nunique'),\n",
    "                Purpose_Match=lambda df_:np.where(df_.Loan_Vol != df_.Purpose_Count,1,0),\n",
    "                Rank_Balance=lambda df_: df_['Balance'].rank(ascending=False, method='min'),\n",
    "                Prov_coll=lambda df_:df_.WeeksInArrears.apply(coll_rates) * df_.Balance,\n",
    "                Prov_final=lambda df_:df_.apply(lambda row: min(sum(row[['Prov_coll', 'ProvisionAmount',]]), row.Balance), axis=1),\n",
    "               )\n",
    "        # .drop(columns=[...,])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d730934-d8df-4f3a-bc57-3c248cde322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function to create the updated DataFrame for analysis\n",
    "df_j1 = tweak_jb(df_j)\n",
    "df_j1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad368e0-15b7-43a1-9992-c217621815bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_j1.Net_bal_review.value_counts()\n",
    "\n",
    "# df_check = (\n",
    "#     df_j1\n",
    "#     .loc[df_j1.Net_bal_review == 0]\n",
    "#     .sample(n=5, random_state=1)\n",
    "# )\n",
    "\n",
    "# df_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb8ae3-ba70-4402-bed7-a822329a35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = (\n",
    "    df_j1\n",
    "    .groupby(['Net_bal_review', 'Write_Off'])\n",
    "    # .Balance.sum()\n",
    "    .Loan_No.count()\n",
    ")\n",
    "df_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3452ba-4655-4d1f-bc5f-71a89b501d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a function to calculate all required statistics\n",
    "# def calculate_statistics(group):\n",
    "#     return pd.Series({\n",
    "#         'count_balance': group['Balance'].count(),\n",
    "#         'sum_balance': group['Balance'].sum(),\n",
    "#         'mean_balance': group['Balance'].mean(),\n",
    "#         'variance_balance': group['Balance'].var(ddof=0),  # Population variance\n",
    "#         'min_balance': group['Balance'].min(),\n",
    "#         'median_balance': group['Balance'].median(),\n",
    "#         'max_balance': group['Balance'].max(),\n",
    "#         'p5_balance': np.percentile(group['Balance'], 5),\n",
    "#         'p95_balance': np.percentile(group['Balance'], 95),\n",
    "#         'skewness_balance': stats.skew(group['Balance']),\n",
    "#         'kurtosis_balance': stats.kurtosis(group['Balance']),\n",
    "        \n",
    "#         'count_shares': group['Attached_Shares'].count(),\n",
    "#         'sum_shares': group['Attached_Shares'].sum(),\n",
    "#         'mean_shares': group['Attached_Shares'].mean(),\n",
    "#         'variance_shares': group['Attached_Shares'].var(ddof=0),  # Population variance\n",
    "#         'min_shares': group['Attached_Shares'].min(),\n",
    "#         'median_shares': group['Attached_Shares'].median(),\n",
    "#         'max_shares': group['Attached_Shares'].max(),\n",
    "#         'p5_shares': np.percentile(group['Attached_Shares'], 5),\n",
    "#         'p95_shares': np.percentile(group['Attached_Shares'], 95),\n",
    "#         'skewness_shares': stats.skew(group['Attached_Shares']),\n",
    "#         'kurtosis_shares': stats.kurtosis(group['Attached_Shares']),\n",
    "#     })\n",
    "\n",
    "# # Group by the binary review flag and apply the statistics function\n",
    "# df_summ_check = df_j1.groupby('Net_bal_review').apply(calculate_statistics).reset_index()\n",
    "\n",
    "# # Display the summary DataFrame\n",
    "# df_summ_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f5411-5fa7-48a1-82aa-97b874790b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Rankings\n",
    "df_top10_nob = (\n",
    "    df_j1\n",
    "    .loc[df_j1.Rank_Balance <= 10]\n",
    "    .sort_values(by='Rank_Balance')\n",
    ")\n",
    "\n",
    "df_top10_nob.shape\n",
    "df_top10_nob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343fd21-ebd7-4cd2-a491-748dcaaac784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMerger:\n",
    "    def __init__(self, df1: pd.DataFrame, df2: pd.DataFrame):\n",
    "        self.df1 = df1\n",
    "        self.df2 = df2\n",
    "        self.merged_df = None\n",
    "\n",
    "    def merge_dataframes(self, on_columns: list, how: str = 'outer', suffix1: str = '_T0', suffix2: str = '_T1') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Merges two DataFrames with specified parameters.\n",
    "\n",
    "        :param on_columns: list - A list of column names to merge on.\n",
    "        :param how: str - The type of merge to be performed. Default is 'outer'.\n",
    "        :param suffix1: str - Suffix to apply to overlapping columns in the first DataFrame. Default is '_T0'.\n",
    "        :param suffix2: str - Suffix to apply to overlapping columns in the second DataFrame. Default is '_T1'.\n",
    "        :return: pd.DataFrame - The merged DataFrame.\n",
    "        \"\"\"\n",
    "        # Perform the merge\n",
    "        df = pd.merge(\n",
    "            self.df1,\n",
    "            self.df2,\n",
    "            how=how,\n",
    "            on=on_columns,\n",
    "            suffixes=(suffix1, suffix2)\n",
    "        )\n",
    "        \n",
    "        # Assign flags for outstanding balances\n",
    "        self.merged_df = df.assign(\n",
    "            T0_Flag=lambda df_: np.where(df_.Balance_T0.notnull(), 1, 0),\n",
    "            T1_Flag=lambda df_: np.where(df_.Balance_T1.notnull(), 1, 0),\n",
    "            Loan_Status=lambda df_: np.where(\n",
    "                (df_.Write_Off_T0 == 1) & (df_.Write_Off_T1 == 1), 'Write_off',\n",
    "                np.where((df_.T0_Flag == 0) & (df_.T1_Flag == 1), 'New',\n",
    "                np.where((df_.T0_Flag == 1) & (df_.T1_Flag == 0), 'Closed', \n",
    "                np.where((df_.T0_Flag == 1) & (df_.T1_Flag == 1), 'Stock', np.nan)))),\n",
    "            Arrears_Band_T0=lambda df_: df_['Arrears_Band_T0'].fillna('Unknown'),  # Replace NaN with 'Unknown'\n",
    "            Arrears_Band_T1=lambda df_: df_['Arrears_Band_T1'].fillna('Unknown'),   # Replace NaN with 'Unknown'\n",
    "            )\n",
    "\n",
    "        # Check that all loans from both DataFrames are present\n",
    "        loans_in_df1 = set(self.df1[on_columns[0]])\n",
    "        loans_in_df2 = set(self.df2[on_columns[0]])\n",
    "        merged_loans = set(self.merged_df[on_columns[0]])\n",
    "\n",
    "        missing_loans_df1_count = len(loans_in_df1 - merged_loans)\n",
    "        missing_loans_df2_count = len(loans_in_df2 - merged_loans)\n",
    "\n",
    "        if missing_loans_df1_count > 0:\n",
    "            print(f\"Count of missing loans from df1: {missing_loans_df1_count}\")\n",
    "        if missing_loans_df2_count > 0:\n",
    "            print(f\"Count of missing loans from df2: {missing_loans_df2_count}\")\n",
    "\n",
    "        return self.merged_df\n",
    "\n",
    "# Example usage\n",
    "df_merger = DataMerger(df_j1, df_s1)\n",
    "df_loan = df_merger.merge_dataframes(on_columns=['Loan_No',])\n",
    "df_loan.shape\n",
    "df_loan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de87dc3d-bc7c-43fa-9c45-e51ef1cfb2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "dups_check_l = df_loan.Loan_No.is_unique\n",
    "dups_check_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e127b-2321-46dc-96a7-6a25da1a95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition of KW parameter provides output for non-numeric features\n",
    "df_loan_description = df_loan.describe(include='all').T\n",
    "df_loan_description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767e3e4-6ae9-4ebd-82eb-9ea622228984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "# Create the summary DataFrame using groupby and aggregate functions in one line\n",
    "# Create the aggregation dictionary\n",
    "agg_dict = {\n",
    "    'loan_count': ('Loan_No', 'count'),\n",
    "    'count': ('Balance_T0', 'count'),\n",
    "    'sum': ('Balance_T0', 'sum'),\n",
    "    'mean': ('Balance_T0', 'mean'),\n",
    "    'variance': ('Balance_T0', lambda x: x.var(ddof=0)),  # Population variance\n",
    "    'min': ('Balance_T0', 'min'),\n",
    "    'median': ('Balance_T0', 'median'),\n",
    "    'max': ('Balance_T0', 'max'),\n",
    "    'p5': ('Balance_T0', lambda x: np.percentile(x, 5)),\n",
    "    'p95': ('Balance_T0', lambda x: np.percentile(x, 95)),\n",
    "    'skewness': ('Balance_T0', lambda x: stats.skew(x)),\n",
    "    'kurtosis': ('Balance_T0', lambda x: stats.kurtosis(x))\n",
    "}\n",
    "\n",
    "# Create the summary DataFrame using groupby and the aggregation dictionary\n",
    "df_summ = (\n",
    "    df_loan\n",
    "    .groupby(['Loan_Status'])\n",
    "    .agg(**agg_dict)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Display the summary DataFrame\n",
    "df_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4468c107-cb36-430a-a82d-569cd50a4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split aggregation dictionaries for T0 and T1\n",
    "agg_dict_T0 = {\n",
    "    'loan_no_T0': ('Loan_No', 'count'),\n",
    "    'count_T0': ('Balance_T0', 'count'),\n",
    "    'sum_ob_T0': ('Balance_T0', 'sum'),\n",
    "    'sum_nb_T0': ('Net_Outstanding_Balance_T0', 'sum'),\n",
    "    'sum_prov_T0': ('Prov_final_T0', 'sum'),\n",
    "    'mean_pc_T0': ('Provision_pct_T0', 'mean'),\n",
    "}\n",
    "\n",
    "agg_dict_T1 = {\n",
    "    'loan_no_T1': ('Loan_No', 'count'),\n",
    "    'count_T1': ('Balance_T1', 'count'),\n",
    "    'sum_ob_T1': ('Balance_T1', 'sum'),\n",
    "    'sum_nb_T1': ('Net_Outstanding_Balance_T1', 'sum'),\n",
    "    'sum_prov_T1': ('Prov_final_T1', 'sum'),\n",
    "    'mean_pc_T1': ('Provision_pct_T1', 'mean'),\n",
    "}\n",
    "\n",
    "# Create a combined summary DataFrame\n",
    "agg_dict_combined = {**agg_dict_T0, **agg_dict_T1}\n",
    "df_combined_summ = (\n",
    "    df_loan\n",
    "    .groupby(['Loan_Status', 'Covered_Loans_T0', 'Covered_Loans_T1'],dropna=False)\n",
    "    .agg(**agg_dict_combined)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Remove rows where count_T0 is zero\n",
    "df_combined_summ = df_combined_summ.loc[(df_combined_summ['loan_no_T0'] > 0) | (df_combined_summ['loan_no_T1'] > 0)]\n",
    "\n",
    "df_combined_summ_a = (\n",
    "    df_loan\n",
    "    .groupby(['Loan_Status', 'Covered_Loans_T0', 'Covered_Loans_T1','Arrears_Band_T0','Arrears_Band_T1'],dropna=False)\n",
    "    .agg(**agg_dict_combined)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Remove rows where count_T0 is zero\n",
    "df_combined_summ_a = df_combined_summ_a.loc[(df_combined_summ_a['loan_no_T0'] > 0) | (df_combined_summ_a['loan_no_T1'] > 0)]\n",
    "\n",
    "# Create the T0 summary DataFrame\n",
    "df_summ_art0a = (\n",
    "    df_loan\n",
    "    .groupby(['Covered_Loans_T0','Arrears_Band_T0'],dropna=False)\n",
    "    .agg(**agg_dict_T0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Remove rows where count_T0 is zero\n",
    "df_summ_art0a = df_summ_art0a.loc[(df_summ_art0a['loan_no_T0'] > 0)]\n",
    "\n",
    "df_summ_art0b = (\n",
    "    df_loan\n",
    "    .groupby(['Loan_Status', 'Covered_Loans_T0', 'Covered_Loans_T1','Arrears_Band_T0'],dropna=False)\n",
    "    .agg(**agg_dict_T0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Remove rows where count_T0 is zero\n",
    "df_summ_art0b = df_summ_art0b.loc[(df_summ_art0b['loan_no_T0'] > 0)]\n",
    "\n",
    "# Create the T1 summary DataFrame\n",
    "df_summ_art1a = (\n",
    "    df_loan\n",
    "    .groupby(['Covered_Loans_T1','Arrears_Band_T1'],dropna=False)\n",
    "    .agg(**agg_dict_T1)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Remove rows where count_T1 is zero\n",
    "df_summ_art1a = df_summ_art1a.loc[(df_summ_art1a['loan_no_T1'] > 0)]\n",
    "\n",
    "df_summ_art1b = (\n",
    "    df_loan\n",
    "    .groupby(['Loan_Status', 'Covered_Loans_T0', 'Covered_Loans_T1','Arrears_Band_T1'],dropna=False)\n",
    "    .agg(**agg_dict_T1)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Remove rows where count_T1 is zero\n",
    "df_summ_art1b = df_summ_art1b.loc[(df_summ_art1b['loan_no_T1'] > 0)]\n",
    "\n",
    "# Export all three DataFrames to one Excel file\n",
    "with pd.ExcelWriter(f'summary_report_{file_run}_{pd.to_datetime(\"today\").date()}.xlsx') as writer:\n",
    "    df_combined_summ.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    df_combined_summ_a.to_excel(writer, sheet_name='Summary_all', index=False)\n",
    "    df_summ_art0a.to_excel(writer, sheet_name='Arrears_Band_T0_Summary', index=False)\n",
    "    df_summ_art0b.to_excel(writer, sheet_name='Arrears_Band_T0_Summaryb', index=False)\n",
    "    df_summ_art1a.to_excel(writer, sheet_name='Arrears_Band_T1_Summary', index=False)\n",
    "    df_summ_art1b.to_excel(writer, sheet_name='Arrears_Band_T1_Summaryb', index=False)\n",
    "    \n",
    "    # Add the DataFrame description and original DataFrame to new sheets\n",
    "    df_loan_description.to_excel(writer, sheet_name='Loan_Description', index=True)\n",
    "    df_loan.to_excel(writer, sheet_name='Loan_Data', index=False)\n",
    "\n",
    "print(f'DataFrames exported to summary_report_{file_run}_{pd.to_datetime(\"today\").date()}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6139af8-8e20-4242-b034-8ce6848ca13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Rates\n",
    "df_loan_filtered = (\n",
    "    df_loan\n",
    "    # .loc[(df_loan.Arrears_Band_T0 != '0') | (df_loan.Arrears_Band_T1 != '0')]\n",
    ")\n",
    "\n",
    "df_rev_t0 = (\n",
    "    pd\n",
    "    .crosstab(index=[df_loan_filtered['Loan_Status'], df_loan_filtered['Arrears_Band_T0']]\n",
    "               ,columns=df_loan_filtered['Arrears_Band_T1']\n",
    "               ,values=df_loan_filtered['Loan_No']\n",
    "               ,aggfunc='count'\n",
    "               ,normalize='index').stack().reset_index().rename(columns={0:'transRate'})\n",
    ")\n",
    "df_rev_t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930608e-79e1-4e7f-af56-8109d1afda8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create all combinations of T0_Flag and T1_Flag\n",
    "# status = df_rev_t0['Loan_Status'].unique()\n",
    "# arr_flags = df_rev_t0['Arrears_Band_T0'].unique()\n",
    "# all_combinations = pd.MultiIndex.from_product(\n",
    "#     [status, arr_flags], \n",
    "#     names=['status', 'Arr_Flag']\n",
    "# )\n",
    "\n",
    "# Pivot table for visualization\n",
    "heatmap_data = df_rev_t0.pivot_table(\n",
    "    index=['Loan_Status','Arrears_Band_T0'], \n",
    "    columns='Arrears_Band_T1', \n",
    "    values='transRate',\n",
    "    # fill_value=0  # Fill missing values with 0\n",
    ")\n",
    "\n",
    "# Visualizing the transRate in a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', cbar=True)\n",
    "plt.title('Heatmap of Transaction Rates for T0 and T1 Flags')\n",
    "plt.xlabel('Arrears Tranche T1')\n",
    "plt.ylabel('T0 and T1 Flags')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f99217d-4075-4b8f-9a64-556798aabed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing movements\n",
    "def var_movement(df, var_cm, var_pm, loan_no='Loan_No', exclude_zeros=False, exclude_negatives=False):\n",
    "    \"\"\"\n",
    "    Analyzes movements between two variables and identifies significant changes.\n",
    "\n",
    "    This function calculates the movement between two specified columns in the DataFrame\n",
    "    (`var_cm` and `var_pm`) and filters the results based on their deviations from the mean.\n",
    "    It can optionally exclude rows where either of the specified columns is zero.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the data.\n",
    "    var_cm (str): The name of the column representing the current value.\n",
    "    var_pm (str): The name of the column representing the previous value.\n",
    "    loan_no (str, optional): The name of the column representing loan numbers. Defaults to 'Loan_No'.\n",
    "    exclude_zeros (bool, optional): If True, excludes rows where var_cm or var_pm are zero. Defaults to False.\n",
    "    exclude_negatives (bool, optional): If True, excludes rows where var_cm or var_pm less than zero. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two DataFrames containing significant movements:\n",
    "        - significant_movements_one_sd: Movements greater than one standard deviation from the mean.\n",
    "        - significant_movements_two_sd: Movements greater than two standard deviations from the mean.\n",
    "    \n",
    "    Visuals:\n",
    "    Displays a histogram of the movements with lines indicating one and two standard deviations,\n",
    "    as well as a QQ plot to assess the normality of the movements.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the val_move variable as the difference between var_cm and var_pm\n",
    "    df['val_move'] = df[var_cm] - df[var_pm]\n",
    "\n",
    "    # Optionally exclude rows where var_cm or var_pm are zero\n",
    "    if exclude_zeros:\n",
    "        df = df[(df[var_cm] != 0) & (df[var_pm] != 0)]\n",
    "        \n",
    "    # Optionally exclude rows where var_cm or var_pm less than zero\n",
    "    if exclude_negatives:\n",
    "        df = df[(df[var_cm] >= 0) & (df[var_pm] >= 0)]\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    mean = df['val_move'].mean()\n",
    "    std_dev = df['val_move'].std()\n",
    "\n",
    "    print(f'Mean: {mean}, Standard Deviation: {std_dev}')\n",
    "\n",
    "    # Use .loc to filter for significant movements using one and two standard deviations\n",
    "    significant_movements_one_sd = df.loc[\n",
    "        (df['val_move'] > (mean + std_dev)) | (df['val_move'] < (mean - std_dev)),\n",
    "        [loan_no, 'val_move', var_cm, var_pm]\n",
    "    ].dropna().sort_values(by='val_move')\n",
    "\n",
    "    significant_movements_two_sd = df.loc[\n",
    "        (df['val_move'] > (mean + 2 * std_dev)) | (df['val_move'] < (mean - 2 * std_dev)),\n",
    "        [loan_no, 'val_move', var_cm, var_pm]\n",
    "    ].dropna().sort_values(by='val_move')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Histogram and KDE with trimmed data\n",
    "    sns.histplot(df['val_move'], bins=30, kde=True, color='lightgray', label='Distribution of val_move', stat='density')\n",
    "    \n",
    "    # Limit the x-axis for better visualization\n",
    "    plt.xlim(mean - 4 * std_dev, mean + 4 * std_dev)\n",
    "\n",
    "    # Add vertical lines for one and two standard deviations\n",
    "    plt.axvline(mean + std_dev, color='blue', linestyle='--', label='Mean + 1 SD')\n",
    "    plt.axvline(mean - std_dev, color='blue', linestyle='--', label='Mean - 1 SD')\n",
    "    plt.axvline(mean + 2 * std_dev, color='red', linestyle='--', label='Mean + 2 SD')\n",
    "    plt.axvline(mean - 2 * std_dev, color='red', linestyle='--', label='Mean - 2 SD')\n",
    "\n",
    "    plt.title('Histogram of val_move with Standard Deviations')\n",
    "    plt.legend()\n",
    "\n",
    "    # QQ Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    stats.probplot(df['val_move'], dist=\"norm\", plot=plt)\n",
    "    plt.title('QQ Plot of val_move')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return the significant movements DataFrames\n",
    "    return significant_movements_one_sd, significant_movements_two_sd\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# one_sd_df, two_sd_df = var_movement(df, 'val_cm', 'val_pm', exclude_zeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d431efb-c6d8-4c89-a7bc-6c9547c0323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net Outstanding Balance\n",
    "one_sd_df, two_sd_df = var_movement(df_loan, 'Net_Balance_T1', 'Net_Balance_T0', exclude_zeros=True)\n",
    "\n",
    "one_sd_df.shape\n",
    "one_sd_df\n",
    "\n",
    "two_sd_df.shape\n",
    "two_sd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab121610-7b13-4796-84f8-ca3405475136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Provision Amount\n",
    "one_sd_df, two_sd_df = var_movement(df_loan, 'Prov_final_T1', 'Prov_final_T0', exclude_zeros=True)\n",
    "\n",
    "one_sd_df.shape\n",
    "one_sd_df\n",
    "\n",
    "two_sd_df.shape\n",
    "two_sd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674171c-ad0f-48aa-9550-cbeadc6ab17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weeks in Arrears\n",
    "one_sd_df, two_sd_df = var_movement(df_loan, 'WeeksInArrears_T1', 'WeeksInArrears_T0', exclude_negatives=True)\n",
    "\n",
    "one_sd_df.shape\n",
    "one_sd_df\n",
    "\n",
    "two_sd_df.shape\n",
    "two_sd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55866f97-3a39-4aa2-b8c7-6858012a485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decile review\n",
    "def decile_summary(df, variable, additional_variable=None):\n",
    "    \"\"\"\n",
    "    Produces a decile summary report for a specified variable in a DataFrame,\n",
    "    with an optional additional variable to show values associated with the decile.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the data.\n",
    "    variable (str): The name of the column for which to calculate deciles.\n",
    "    additional_variable (str, optional): An additional variable to be summarized with respect to the deciles.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A summary DataFrame containing decile information.\n",
    "    \"\"\"\n",
    "    # Calculate deciles\n",
    "    deciles = np.percentile(df[variable].dropna(), np.arange(0, 101, 10))\n",
    "    \n",
    "    # Create bins based on the deciles\n",
    "    df['Decile'] = pd.cut(df[variable], bins=deciles, include_lowest=True, labels=np.arange(1, 11))\n",
    "    \n",
    "    # Prepare aggregation dictionaries\n",
    "    agg_dict = {\n",
    "        'Count': ('Decile', 'size'),\n",
    "        'Sum': (variable, 'sum'),\n",
    "        'Mean': (variable, 'mean'),\n",
    "        'Std_Dev': (variable, 'std'),\n",
    "        'Min': (variable, 'min'),\n",
    "        'Max': (variable, 'max'),\n",
    "    }\n",
    "\n",
    "    # If an additional variable is provided, add it to the aggregation\n",
    "    if additional_variable:\n",
    "        agg_dict[f'Count_GT_0_{additional_variable}'] = (additional_variable, lambda x: (x > 0).sum())\n",
    "        agg_dict[f'Mean_{additional_variable}'] = (additional_variable, 'mean')\n",
    "        agg_dict[f'Sum_{additional_variable}'] = (additional_variable, 'sum')\n",
    "        agg_dict[f'Std_Dev_{additional_variable}'] = (additional_variable, 'std')\n",
    "\n",
    "    # Group by Decile and calculate summary statistics\n",
    "    summary = df.groupby('Decile').agg(**agg_dict).reset_index()\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "# Sample DataFrame with multiple variables\n",
    "# Get the decile summary with an additional variable\n",
    "# decile_report = decile_summary(df, 'val_move', additional_variable='another_variable')\n",
    "# print(decile_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4074773-b338-47ba-a1f8-15df68b6909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_report_1 = decile_summary(df_loan, 'Balance_T1', 'Prov_final_T1')\n",
    "decile_report_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30068350-1b64-486c-999e-42564178a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create scatter graphs with and without filtering\n",
    "def create_scatter_graph(df, x_variable, y_variable, z_variable, filter_values=None):\n",
    "    \"\"\"\n",
    "    Creates scatter graphs for the specified x and y variables, with the z variable\n",
    "    determining the color of the points. Displays two plots: one with all data and\n",
    "    another filtered by the specified values in the z variable.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the data.\n",
    "    x_variable (str): The name of the column for the x-axis.\n",
    "    y_variable (str): The name of the column for the y-axis.\n",
    "    z_variable (str): The name of the column for coloring the points.\n",
    "    filter_values (list, optional): A list of values in the z variable to filter out.\n",
    "    \"\"\"\n",
    "    # Plot all data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(data=df, x=x_variable, y=y_variable, hue=z_variable, palette='viridis', style=z_variable)\n",
    "    plt.title('Scatter Graph of Balance vs Provision Amount (All Data)')\n",
    "    plt.xlabel(x_variable)\n",
    "    plt.ylabel(y_variable)\n",
    "    plt.legend(title=z_variable)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot filtered data if filter_values is provided\n",
    "    if filter_values:\n",
    "        filtered_df = df[~df[z_variable].isin(filter_values)]\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.scatterplot(data=filtered_df, x=x_variable, y=y_variable, hue=z_variable, palette='viridis', style=z_variable)\n",
    "        plt.title(f'Scatter Graph (Excluding {\", \".join(filter_values)})')\n",
    "        plt.xlabel(x_variable)\n",
    "        plt.ylabel(y_variable)\n",
    "        plt.legend(title=z_variable)\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "create_scatter_graph(df_loan, 'Balance_T0', 'Prov_final_T0', 'Arrears_Band_T0', filter_values=['0','1-9'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4cf27e-c2e4-4b83-b010-fdedc8b5382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create scatter graphs with filtering and colored bounding boxes\n",
    "def create_scatter_graph_p(df, x_variable, y_variable, z_variable, filter_values=None):\n",
    "    \"\"\"\n",
    "    Creates scatter graphs for the specified x and y variables, with the z variable\n",
    "    determining the color of the points. Displays two plots: one with all data and\n",
    "    another filtered by the specified values in the z variable, with bounding boxes around cohorts.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the data.\n",
    "    x_variable (str): The name of the column for the x-axis.\n",
    "    y_variable (str): The name of the column for the y-axis.\n",
    "    z_variable (str): The name of the column for coloring the points.\n",
    "    filter_values (list, optional): A list of values in the z variable to filter out.\n",
    "    \"\"\"\n",
    "    # Set a color palette\n",
    "    palette = sns.color_palette(\"viridis\", len(df[z_variable].unique()))\n",
    "    \n",
    "    # Plot all data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(data=df, x=x_variable, y=y_variable, hue=z_variable, palette='viridis', style=z_variable)\n",
    "    plt.title('Scatter Graph of Balance vs Provision Amount (All Data)')\n",
    "    plt.xlabel(x_variable)\n",
    "    plt.ylabel(y_variable)\n",
    "    plt.legend(title=z_variable)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot filtered data if filter_values is provided\n",
    "    if filter_values:\n",
    "        filtered_df = df[~df[z_variable].isin(filter_values)]\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        scatter = sns.scatterplot(data=filtered_df, x=x_variable, y=y_variable, hue=z_variable, palette='viridis', style=z_variable)\n",
    "        plt.title(f'Scatter Graph (Excluding {\", \".join(filter_values)})')\n",
    "        plt.xlabel(x_variable)\n",
    "        plt.ylabel(y_variable)\n",
    "        plt.legend(title=z_variable)\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Draw bounding boxes around the different cohorts\n",
    "        for i, cohort in enumerate(df[z_variable].unique()):\n",
    "            if cohort not in filter_values:\n",
    "                cohort_data = filtered_df[filtered_df[z_variable] == cohort]\n",
    "                if not cohort_data.empty:\n",
    "                    x_min, x_max = cohort_data[x_variable].min(), cohort_data[x_variable].max()\n",
    "                    y_min, y_max = cohort_data[y_variable].min(), cohort_data[y_variable].max()\n",
    "                    \n",
    "                    # Create a rectangle patch with color from the palette\n",
    "                    rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                                             linewidth=2, edgecolor=palette[i], facecolor='none', linestyle='--')\n",
    "                    plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "create_scatter_graph_p(df_loan, 'Balance_T0', 'Prov_final_T0', 'Arrears_Band_T0', filter_values=['0',])\n",
    "create_scatter_graph_p(df_loan, 'Balance_T0', 'Prov_final_T0', 'Arrears_Band_T0', filter_values=['0','1-9'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
